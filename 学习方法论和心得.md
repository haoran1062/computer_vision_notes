# CV算法学习方法论和心得
## 概述
* 从大学入门cv开始一直处于野生自学状态，走了很多的弯路，如今摸索了一些学习算法的心得，在此记录一下，如果大家有更好的学习方式可以在issue里提一下，欢迎大家一起讨论，共同学习成长。

## 学习方法论
* 首先，看到一篇新出炉的算法，对于我这样一个以工程落地为主的工程师来说，首先要看一下paper有没有人进行解读，如果没有人解读，那就只能先硬着头皮自己看paper了。当然，不管怎么样都是要自己看一遍才行的，因为每个人对paper的解读都是不一样的，说实话解读质量也是参差不齐的，但是大致对的情况下还是很利于快速了解该算法的原理的。在此基础上再自己看一下paper，增进一下对算法的理解。
* 其次，最重要的就是看论文有没有给出源码（不给源码都是刷流氓 233）：
    * 1.有就第一时间`git clone`下来
    * 2.看readme
    * 3.准备数据集先跑起来，看看有没有什么bug
    * 4.看实际训练结果是否可以和paper结果吻合。
    * 5.训练自己的数据集，看一下训练效果和目前在用模型做比较，这时候要耐心，根据之前了解的算法原理多试几组参数，不要因为懒惰而得出错误结论，曾经就因为懒得多做几组参数对比而对某些优秀算法
    * 5.还ok的话就可以仔细看代码了（功利的说，如果效果不如目前使用的算法好，那还不如花精力在其他模型上面，当然如果算法本身有很惊奇的思路，是可以好好研究一番，看看有没有改进的地方使其达到工业场景可用。）
* 看源码的方法
    * 首先，先run起来，如果官方给了weights，加载一下看看效果，如果没有给权重文件，就可以先看如何定义数据，数据加载格式，准备好自己的数据，再跑一下训练。这时候肯定要看过`dataloader`和`config`了。看`dataloader`的时候就可以学习人家是怎么设计数据加载的，数据经过了什么样的`data augument`等，`config`就可以大致看到网络如何设计，由什么模块组成，方便之后详细的源码剖析。下面我总结一下我看到的优秀开源项目大致构成，我自己写的repo也会参考这套标准来做。
    * 一般CV算法项目结构
        * `config`
            * 定义`算法结构`，`各模块参数`，`加载数据相关`以及`train和evaluate相关设置`等。
        * `dataloader`
            * 定义数据加载相关，例如`输入数据格式format`，`encoder`, `decoder`以及`数据增广`等
        * `backbone`
            * 没啥好说的，几乎没见过不需要`backbone`的`deep learning`的`CV`项目
        * `neck`（类型很多，根据算法方向不同，本身算法特性不同会有很多种`neck`）
            * `FPN`,`FPEM`,`FFM`等**特征融合层类型**的`neck`
            * 类似`ROI pooling(align)`这种自定义网络层
            * `detect head(layer)` 很多目标检测算法都会有检测头类型的`neck`，例如`YOLOLayer`之类的
            * `recognition head` 很多文本识别算法都会有文字识别类型的`neck`，例如`CRNN`,`attention-based`文本识别算法会有。
            * 其他，太多了就不列举了，在CV界复杂些的算法(像目标检测，文本识别，分割等)在同样性能的`backbone`下基本上就是靠`neck`和`loss function`（有些还包括`post process`）来涨点，这些经常是理解算法思想的重点模块，**需要仔细看(划重点！！！)**。
        * `post process`
            * 常见于有`encoder-decoder`思想的算法
            * 常见于基于`分割`的`文本检测算法`，例如`PSENet`, `PAN`, `PAN++`等，这种后处理算法有很大一部分是借鉴参考上古CV传统算法灵活运用魔改而来，包括某些`neck`也是（例如`ROI pooling`, `ROI align`）
        * `loss function`
            * 很多复杂模型（常见于多个模块，`multi-head`这种）的`loss function`会设计的很巧妙，因为要结合并权衡各模块的loss，但是由于系统过于繁杂，loss设计也很难简洁，loss的设计的确是模型最重要的一环。
        * `tools(utils)`
            * `image tools`: 包含`image2tensor`,`padding resize`等对`numpy`, `opencv`, `Pillow`，`Tensor`图像处理的工具。
            * `NMS`, `soft NMS`等`算子`
        * `scripts`
            * `trainer` 训练脚本
            * `eval` 测试脚本
            * `data format`等杂七杂八的脚本
        * `其他`
            * 可视化服务：`visdom`，`tensorboard`等，我个人喜欢用`visdom`(pytorch铁杆用户)
            * log系统 （有些比较学术的大佬直接`print`，建议碰到这种还是改成`logger`，至少训练结果有个记录，如果直接把stdout重定向到文件有点挫，而且会缺少训练时间等信息。这里推荐使用`rich`包一下`logger` 美观又实用）

    * 如何快速读代码来理解算法？答案就是：动手让`tensor` `flow`起来！
        * 动手看代码方式有两种：跑`train`和`evaluate`代码
        * 如果想粗略了解算法框架, 跑`evaluate`基本上就够了，而如果想深入理解算法原理，理解`loss function`设计思路，学习数据编解码处理，还是要`run train.py`
        * 不管是哪种，方法都一样，只需要debug打点来看代码，这样可以按顺序快速的，一个一个模块的解析算法模型是如何设计构造来`work together`，由于我是`vscode`党，所以在此推荐安装`vscode`的`python`, `bookmark`, `docker`, `remote-ssh`, `remote-container`, `remote-development`, `sftp`这些插件，这样就可以直接在远程服务器中的`docker container`中`debug`打点看代码了。当然如果你可以直接在GPU机器上debug，也不用这么麻烦了，直接在宿主机的`conda env` debug也行（毕竟确实是方便很多），但还是推荐在docker环境下搞，毕竟工程师还是要注意环境分离，安全第一。
        * 打点推荐
            * 第一，在`training`入口脚本中，分别找`config`, `dataloader`定义处打点, 再找`train loop`中的`xx = model(input)`打点，往后找`loss = compute_loss_function(xx)`再打个点。
            * 然后`F5`开始debug
            * 在上面提到打点的地方进入到各个函数或数据定义的`python文件`处，浏览一下该断点用到的函数，在需要打点的地方再分别打点，挨个运行，边运行边看函数的构成和当前`data`在graph中`tensor` `flow`（不得不说google会起名字）的情况，基本上跑几个`batch`就可以很快理解整个算法了！
* 上面就是我个人总结的快速看源码深入理解算法的方法心得，后面有时间的话会更新一些关于算法工程化优化的东西，大家如果有问题或者好的想法可以在issue区一起交流讨论哈～