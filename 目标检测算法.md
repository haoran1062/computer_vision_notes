# 目标检测算法

根据目标检测原理大致可以先根据**两阶段**和**单阶段**划分，两阶段一般精度较高但计算量较大pipline更复杂所以速度更慢，单阶段模型pipline简单优雅，但精度一般稍逊一筹，随着object detection发展，单阶段模型精度已经慢慢达到甚至超越双阶段的baseline。后期又发展出`anchor-base`和`anchor-free`（主要是单阶段，孤陋寡闻没听说双阶段有anchor-free的）的算法。近期由于`transformer`的跨界，也有很多更强的新模型出现，在cv界大杀四方，靠暴力的参数量疯狂屠榜（半开玩笑，误！），由于工作主要还是对落地有较高要求（主要是性价比），本人暂无详细研究（2019年底我又回归OCR大业，主要搞回文本检测，文本识别，轻量化网络等方面的东西去了，所以后面百花齐放的通用目标检测算法也只有纸上谈兵了），等待后续钻研。

## 两阶段与单阶段大致区别

字面意思，两阶段是将目标检测分为两大部分，简单来说第一阶段是基于分类思想，先把前景和目标分离开，第二阶段再精调目标框。单阶段是一步到位，基本上就是以回归的思想直接把目标检测的活一次干完，两者的算法理念不同，各有优缺点。随着时间发展，两阶段的成本问题以及单阶段的性能不断提升，工业场景基本上都是单阶段目标检测模型落地了（据我所知）。

## 两阶段

* 发展大致从RCNN -> fast RCNN -> faster RCNN -> Cascade RCNN
* 由于RCNN和fast-RCNN现在基本上没人用了，就简单介绍一下，faster-rcnn及其后续的mask-rcnn和cascade-rcnn才是工业界用到的成熟算法模型，而且由于单阶段模型的百花齐放，现在也罕见（个人所见 也许是以偏概全了😂）工业场景用两阶段目标检测落地了（主要是成本控制和效率问题，mask-rcnn做实例分割还是见过几家在用的，现在有没有在用就不晓得了）

### RCNN

* 原理&pipline
  * ![pipline图解](images/RCNN_pipline.jpeg)
  * 图解中第二步：首先通过selective search从输入图像中得到2k个预选框，由于该方法过于古老，目前已经没有任何优势，贴一个很棒的算法简介[selective search](https://zhuanlan.zhihu.com/p/39927488)我就不多赘述了。
  * 图解中的warped region：就是把通过selective search得到的2k个框内图像resize到同一个尺寸
  * 图解第三步：把resize后到图像扔到CNN里，将CNN最后pool层的特征保存下来
  * 第四步： 用SVM对第三步抽的feature进行分类和回归；正样本与Ground Truth的IoU>0.5，最大IoU；负样本IoU<0.3，这里是训练多个二分类的SVM分类器
  * bbox的回归：训练一个具有校正因子的线性回归分类器(FC)，损失函数是L2损失。 回归分类器预测的是Ground Truth相对于`预选框`的中心点、长宽的偏差，其中G表示Ground Truth，P表示预选框(Proposal)。中心点使用相对坐标便于优化(区间在0~1)，并且可以很好的应对不同尺度的目标、图像大小；长宽用log，是因为G=P exp(dP)，反过来就是log了。
  * ![bbox回归function](images/RCNN_bbox_loss_function_2.png)
  * ![bbox回归function2](images/RCNN_bbox_loss_function_1.jpeg)

* 贡献&亮点
  * selective search
  * 早期使用CNN做目标检测有比较好效果的算法模型，也是奠定了RCNN系列两阶段目标检测模型的开山之作。膜拜RGB大神和凯明大神！
* 不足
  * 不能e2e的训练，训练很繁琐，由于是早期模型，速度和精度都很低，记得很早之前试过要分钟级别的evaluate才能出结果。。。
  

### fast-RCNN

* 原理&pipline
  * 在RCNN基础上改进，只需要对整幅图像（而不是对每个bbox内的小图）进行一次特征提取，并且直接利用FC进行分类和回归，实现end-to-end两阶段目标检测。
  * 具体过程：
    * ![fast-rcnn pipline](images/fast-rcnn_pipline.jpeg)
    * 1.图像先经过Backbone网络提取图像的feature map
    * 2.利用selective search搜索的预选框到特征上提取RoI的特征
    * 3.通过RoI Pooling将RoI的特征转化为固定的大小
    * 4.最后输入到分类和回归头分别进行分类和回归
* 贡献&亮点
  * Roi pooling
    * RoI Pooling的有两个输入，一个是feature map，即整个图像经过Backbone提取的特征；另一个是RoI的坐标5维，[idx，x1，y1，x2，y2]表示左上角和右下角的坐标。坐标尺度是相对原图的尺度，而在feature上根据图像下采样的比例(即feature/原图尺度)找到对应的坐标。
    * 将映射后的区域划分为相同大小的区间(比如7×7)，这样就可以从不同大小的RoI中获得固定大小的feature map，最终输出[H,W,C]维度的特征图，C表示通道个数，然后进行分类和回归处理。 值得注意的是： 当proposal在特征图上的尺度大于7×7时(比如RoI Pooling输出的是7×7)，比较好理解，直接计算特征图到7×7映射的每个bin的最大值；当proposal小于7×7咋办呢？其实还是一样的，仍然这样强行处理，只不过会有一部分值是重复的。在Mask RCNN中对RoI Pooling进行了修改，采用线性插值的方式来做的，也就是roi align，工业场景有直接用roi align替换roi pooling使faster-rcnn更准。
    * 损失函数 损失函数分为分类和回归损失函数，与RCNN不同的是这里直接用FC进行分类和回归，分类损失函数Cross Entropy和Smooth L1。 分类共有N+1类，+1表示背景类； 回归用Smooth L1损失，对[x，y，w，h]进行回归，其中回归的约束目标仍然是中心点和长宽的相对值，与RCNN相似。Smooth L1损失：
    * ![bbox 回归function](images/smoothL1.jpeg)
  * 整个过程e2e
* 不足
  * 候选框仍基于selective search，尽管比RCNN快点，但还是很慢
* 源码
  * 推荐[mmdetection实现](https://github.com/open-mmlab/mmdetection)

### faster-RCNN

* 原理&pipline
  * Faster RCNN在Fast RCNN的基础上提出RPN(Region Proposal Network)自动生成RoI，极大的提高了预选框生成的效率，取代selective search，之后还是fast-rcnn
* 贡献&亮点
  * anchor-based
    * ![anchor generator](images/anchor.jpeg)
    * 提出了在feature map上画密集anchor的方式生成规则候选框，是很暴力，但是效果也很好
    * 在这里，以feature map的每一个点为中心，以不同长宽比(1:1, 1:2, 2:1)，缩放比(8, 16, 32倍)为规则生成anchor box，如果应用在类似OCR这种长宽比比较奇葩的场景，可以根据具体场景gt bbox的分布适当修改这两个参数，以生成更适合的anchor。
  * RPN
    * ![RPN](images/RPN.jpeg)
    * RPN网络输入是feature map，输出是RoI及分类结果,此时只做背景和目标二分类
    * RPN网络计算所有生成的`RoI与anchor的偏差`和`是否为object的分类`结果
    * 对`是否为object分类`结果进行分数排序，获得前N个预选框，记得paper里是`2k`个？
    * 对超出边界的bbox进行越界处理
    * 对候选框们进行`NMS`去重，这里简单说一下`NMS`：大致就是对所有框`IOU`大于阈值的进行去重处理，`IOU`和`NMS`这种算子会在末尾说明。
    * 最后将RPN网络预测的RoI送入到RoI Pooling中进行特征提取，此特征与输入RPN网络的特征图是同一个，后续接分类头和回归头，`简单来说faster-rcnn基本上就是用RPN代替selective search的fast-rcnn，也就等同于RPN+fast-rcnn`
* 源码
  * 推荐[mmdetection实现](https://github.com/open-mmlab/mmdetection)

### Cascade-RCNN

* 原理&pipline
* 贡献&亮点
* 源码
  * 推荐[mmdetection实现](https://github.com/open-mmlab/mmdetection)


## 单阶段

发展大致从YOLO v1 -> SSD -> YOLO v2 -> YOLO v3 -> RetinaNet -> FCOS -> CenterNet -> YOLO v4，v5，YOLOF，YOLOR，YOLOX，YOLOE等各种花式YOLO变体（大部分还是叠加工程trick，算法大改进没感觉有啥）以及基于transformer（应该算是单阶段？）等。

### YOLO v1

* 原理&pipline
* 贡献&亮点
* 源码

### YOLO v2

* 原理&pipline
* 贡献&亮点
* 源码

### YOLO v3

* 原理&pipline
* 贡献&亮点
* 源码

### SSD

* 原理&pipline
* 贡献&亮点
* 源码

### RetinaNet

* 原理&pipline
* 贡献&亮点
* 源码

### FCOS

* 原理&pipline
* 贡献&亮点
* 源码

### CenterNet

* 原理&pipline
* 贡献&亮点
* 源码

### YOLO v4

* 原理&pipline
* 贡献&亮点
* 源码

### YOLO v5

* 原理&pipline
* 贡献&亮点
* 源码

### YOLOF

* 原理&pipline
* 贡献&亮点
* 源码

### YOLOR

* 原理&pipline
* 贡献&亮点
* 源码

### YOLOX

* 原理&pipline
* 贡献&亮点
* 源码

### YOLOE

* 原理&pipline
* 贡献&亮点
  * baidu集各种工程trick的集大成之作，可以说是工业上拿来即用的神器了
* 源码



## Transformer
### 待学习待研究。。。

## 目标检测通用算子和架构
* NMS及其变种
* FPN