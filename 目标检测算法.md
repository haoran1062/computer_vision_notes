# 目标检测算法

根据目标检测原理大致可以先根据**两阶段**和**单阶段**划分，两阶段一般精度较高但计算量较大pipline更复杂所以速度更慢，单阶段模型pipline简单优雅，但精度一般稍逊一筹，随着object detection发展，单阶段模型精度已经慢慢达到甚至超越双阶段的baseline。后期又发展出`anchor-base`和`anchor-free`（主要是单阶段，孤陋寡闻没听说双阶段有anchor-free的）的算法。近期由于`transformer`的跨界，也有很多更强的新模型出现，在cv界大杀四方，靠暴力的参数量疯狂屠榜（半开玩笑，误！），由于工作主要还是对落地有较高要求（主要是性价比），本人暂无详细研究（2019年底我又回归OCR大业，主要搞回文本检测，文本识别，轻量化网络等方面的东西去了，所以后面百花齐放的通用目标检测算法也只有纸上谈兵了），等待后续钻研。

## 两阶段与单阶段大致区别

字面意思，两阶段是将目标检测分为两大部分，简单来说第一阶段是基于分类思想，先把前景和目标分离开，第二阶段再精调目标框。单阶段是一步到位，基本上就是以回归的思想直接把目标检测的活一次干完，两者的算法理念不同，各有优缺点。随着时间发展，两阶段的成本问题以及单阶段的性能不断提升，工业场景基本上都是单阶段目标检测模型落地了（据我所知）。

## 两阶段

* 发展大致从RCNN -> fast RCNN -> faster RCNN -> Cascade RCNN
* 由于RCNN和fast-RCNN现在基本上没人用了，就简单介绍一下，faster-rcnn及其后续的mask-rcnn和cascade-rcnn才是工业界用到的成熟算法模型，而且由于单阶段模型的百花齐放，现在也罕见（个人所见 也许是以偏概全了😂）工业场景用两阶段目标检测落地了（主要是成本控制和效率问题，mask-rcnn做实例分割还是见过几家在用的，现在有没有在用就不晓得了）

### RCNN

* 原理&pipline
  * ![pipline图解](images/RCNN_pipline.jpeg)
  * 图解中第二步：首先通过selective search从输入图像中得到2k个预选框，由于该方法过于古老，目前已经没有任何优势，贴一个很棒的算法简介[selective search](https://zhuanlan.zhihu.com/p/39927488)我就不多赘述了。
  * 图解中的warped region：就是把通过selective search得到的2k个框内图像resize到同一个尺寸
  * 图解第三步：把resize后到图像扔到CNN里，将CNN最后pool层的特征保存下来
  * 第四步： 用SVM对第三步抽的feature进行分类和回归；正样本与Ground Truth的IoU>0.5，最大IoU；负样本IoU<0.3，这里是训练多个二分类的SVM分类器
  * bbox的回归：训练一个具有校正因子的线性回归分类器(FC)，损失函数是L2损失。 回归分类器预测的是Ground Truth相对于`预选框`的中心点、长宽的偏差，其中G表示Ground Truth，P表示预选框(Proposal)。中心点使用相对坐标便于优化(区间在0~1)，并且可以很好的应对不同尺度的目标、图像大小；长宽用log，是因为G=P exp(dP)，反过来就是log了。
  * ![bbox回归function](images/RCNN_bbox_loss_function_2.png)
  * ![bbox回归function2](images/RCNN_bbox_loss_function_1.jpeg)

* 贡献&亮点
  * selective search
  * 早期使用CNN做目标检测有比较好效果的算法模型，也是奠定了RCNN系列两阶段目标检测模型的开山之作。膜拜RGB大神和凯明大神！
* 不足
  * 不能e2e的训练，训练很繁琐，由于是早期模型，速度和精度都很低，记得很早之前试过要分钟级别的evaluate才能出结果。。。
  

### fast-RCNN

* 原理&pipline
  * 在RCNN基础上改进，只需要对整幅图像（而不是对每个bbox内的小图）进行一次特征提取，并且直接利用FC进行分类和回归，实现end-to-end两阶段目标检测。
  * 具体过程：
    * ![fast-rcnn pipline](images/fast-rcnn_pipline.jpeg)
    * 1.图像先经过Backbone网络提取图像的feature map
    * 2.利用selective search搜索的预选框到特征上提取RoI的特征
    * 3.通过RoI Pooling将RoI的特征转化为固定的大小
    * 4.最后输入到分类和回归头分别进行分类和回归
* 贡献&亮点
  * Roi pooling
    * RoI Pooling的有两个输入，一个是feature map，即整个图像经过Backbone提取的特征；另一个是RoI的坐标5维，[idx，x1，y1，x2，y2]表示左上角和右下角的坐标。坐标尺度是相对原图的尺度，而在feature上根据图像下采样的比例(即feature/原图尺度)找到对应的坐标。
    * 将映射后的区域划分为相同大小的区间(比如7×7)，这样就可以从不同大小的RoI中获得固定大小的feature map，最终输出[H,W,C]维度的特征图，C表示通道个数，然后进行分类和回归处理。 值得注意的是： 当proposal在特征图上的尺度大于7×7时(比如RoI Pooling输出的是7×7)，比较好理解，直接计算特征图到7×7映射的每个bin的最大值；当proposal小于7×7咋办呢？其实还是一样的，仍然这样强行处理，只不过会有一部分值是重复的。在Mask RCNN中对RoI Pooling进行了修改，采用线性插值的方式来做的，也就是roi align，工业场景有直接用roi align替换roi pooling使faster-rcnn更准。
    * 损失函数 损失函数分为分类和回归损失函数，与RCNN不同的是这里直接用FC进行分类和回归，分类损失函数Cross Entropy和Smooth L1。 分类共有N+1类，+1表示背景类； 回归用Smooth L1损失，对[x，y，w，h]进行回归，其中回归的约束目标仍然是中心点和长宽的相对值，与RCNN相似。Smooth L1损失：
    * ![bbox 回归function](images/smoothL1.jpeg)
  * 整个过程e2e
* 不足
  * 候选框仍基于selective search，尽管比RCNN快点，但还是很慢
* 源码
  * 推荐[mmdetection实现](https://github.com/open-mmlab/mmdetection)

### faster-RCNN

* 原理&pipline
  * Faster RCNN在Fast RCNN的基础上提出RPN(Region Proposal Network)自动生成RoI，极大的提高了预选框生成的效率，取代selective search，之后还是fast-rcnn
* 贡献&亮点
  * anchor-based
    * ![anchor generator](images/anchor.jpeg)
    * 提出了在feature map上画密集anchor的方式生成规则候选框，是很暴力，但是效果也很好
    * 在这里，以feature map的每一个点为中心，以不同长宽比(1:1, 1:2, 2:1)，缩放比(8, 16, 32倍)为规则生成anchor box，如果应用在类似OCR这种长宽比比较奇葩的场景，可以根据具体场景gt bbox的分布适当修改这两个参数，以生成更适合的anchor。
  * RPN
    * ![RPN](images/RPN.jpeg)
    * RPN网络输入是feature map，输出是RoI及分类结果,此时只做背景和目标二分类
    * RPN网络计算所有生成的`RoI与anchor的偏差`和`是否为object的分类`结果
    * 对`是否为object分类`结果进行分数排序，获得前N个预选框，记得paper里是`2k`个？
    * 对超出边界的bbox进行越界处理
    * 对候选框们进行`NMS`去重，这里简单说一下`NMS`：大致就是对所有框`IOU`大于阈值的进行去重处理，`IOU`和`NMS`这种算子会在末尾说明。
    * 最后将RPN网络预测的RoI送入到RoI Pooling中进行特征提取，此特征与输入RPN网络的特征图是同一个，后续接分类头和回归头，`简单来说faster-rcnn基本上就是用RPN代替selective search的fast-rcnn，也就等同于RPN+fast-rcnn`
* 源码
  * 推荐[mmdetection实现](https://github.com/open-mmlab/mmdetection)

### Cascade-RCNN

* 原理&pipline
  * Faster RCNN进行改进的版本, 由于faster-rcnn训练时知道Ground Truth（GT），可以直接筛选高质量的Proposal用于最后的分类和回归，但是在测试阶段，我们并不知道GT，导致所有的Proposal都进行最后的分类和定位，这就是训练和测试阶段的不匹配问题，所以提出使用集联的多个检测头，每个检测头的IoU呈现递增的情况，比如0.5、0.6、0.7。低级检测头采用低IoU阈值可以提高召回率，避免目标丢失；后续的高级检测头在前一阶段的基础之上提高阈值可以提高检测精度。这种方法可以在增加不太多参数量的基础上提升检测器的精度（反正都很慢了，如果用于工业场景也是对速度没啥要求，对精度有很高要求的情况，所以再慢点也可以接受吧。。。）
  * ![cascade detect head](images/cascade_rcnn.jpeg)
* 贡献&亮点
  * cascade多个检测头，提升mAP
* 源码
  * 推荐[mmdetection实现](https://github.com/open-mmlab/mmdetection)


## 单阶段

发展大致从YOLO v1 -> SSD -> YOLO v2 -> YOLO v3 -> RetinaNet -> FCOS -> CenterNet -> YOLO v4，v5，YOLOF，YOLOR，YOLOX，YOLOE等各种花式YOLO变体（大部分还是叠加工程trick，算法大改进没感觉有啥）以及基于transformer（应该算是单阶段？）等。

### YOLO v1

* 原理&pipline
* 贡献&亮点
* 源码

### YOLO v2

* 原理&pipline
* 贡献&亮点
* 源码

### YOLO v3

* 原理&pipline
* 贡献&亮点
* 源码

### SSD

* 原理&pipline
* 贡献&亮点
* 源码

### RetinaNet

* 原理&pipline
* 贡献&亮点
* 源码

### FCOS

* 原理&pipline
* 贡献&亮点
* 源码

### CenterNet

* 原理&pipline
* 贡献&亮点
* 源码

### YOLO v4

* 原理&pipline
* 贡献&亮点
* 源码

### YOLO v5

* 原理&pipline
* 贡献&亮点
* 源码

### YOLOF

* 原理&pipline
* 贡献&亮点
* 源码

### YOLOR

* 原理&pipline
* 贡献&亮点
* 源码

### YOLOX

* 原理&pipline
* 贡献&亮点
* 源码

### YOLOE

* 原理&pipline
* 贡献&亮点
  * baidu集各种工程trick的集大成之作，可以说是工业上拿来即用的神器了
* 源码



## Transformer
### 待学习待研究。。。

## 目标检测通用算子和架构
* NMS及其变种
* FPN

## 参考文章
* [单阶段、两阶段目标检测经典算法(RCNN系列、YOLO系列、SSD、RetinaNet等)大汇总](https://zhuanlan.zhihu.com/p/367069340)